# -*- coding: utf-8 -*-
"""
è§†é¢‘åˆ†æå™¨ - é‡æ„ç‰ˆæœ¬
åŠŸèƒ½ï¼šè§†é¢‘å†…å®¹åˆ†æã€åœºæ™¯æ£€æµ‹ã€è¯­éŸ³è¯†åˆ«ã€å¯¹è±¡æ£€æµ‹ç­‰
"""

import subprocess
import json
import os
import cv2
import speech_recognition as sr
import librosa
import numpy as np
import whisper
from ultralytics import YOLO
from typing import Dict, Any, List, Optional

from core.utils.config_manager import config, ErrorHandler, PathHelper

try:
    # æ–°ç‰ˆæœ¬ PySceneDetect API (æ¨è)
    from scenedetect import open_video, SceneManager, ContentDetector

    NEW_API_AVAILABLE = True
except ImportError:
    try:
        # æ—§ç‰ˆæœ¬ API (å‘åå…¼å®¹)
        from scenedetect import VideoManager, SceneManager
        from scenedetect.detectors import ContentDetector

        NEW_API_AVAILABLE = False
    except ImportError:
        print("âŒ PySceneDetect æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸å…¼å®¹")
        NEW_API_AVAILABLE = None


class VideoAnalyzer:
    def __init__(self):
        # æŠ‘åˆ¶PySceneDetectçš„å¼ƒç”¨è­¦å‘Š
        import warnings
        warnings.filterwarnings("ignore", message=".*VideoManager.*deprecated.*", category=DeprecationWarning)

        self.yolo_model = None
        self._whisper_model = None
        self.speech_timestamps = {}  # æ–°å¢ï¼šå­˜å‚¨æ—¶é—´æˆ³æ•°æ®

    @property
    def whisper_model(self):
        """æ‡’åŠ è½½Whisperæ¨¡å‹"""
        if self._whisper_model is None:
            try:
                print("[+] åŠ è½½Whisperæ¨¡å‹...")
                self._whisper_model = whisper.load_model("base")
                print("[+] Whisperæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self._whisper_model = False
        return self._whisper_model if self._whisper_model is not False else None

    def detect_videos(self, input_path):
        """æ£€æµ‹è§†é¢‘æ•°é‡å’Œè·¯å¾„"""
        video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv')

        if os.path.isfile(input_path):
            if input_path.lower().endswith(video_extensions):
                return [input_path]
            else:
                raise ValueError(f"ä¸æ˜¯æœ‰æ•ˆçš„è§†é¢‘æ–‡ä»¶: {input_path}")
        elif os.path.isdir(input_path):
            video_files = [
                os.path.join(input_path, f)
                for f in os.listdir(input_path)
                if f.lower().endswith(video_extensions)
            ]
            if not video_files:
                raise ValueError(f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {input_path}")
            return video_files
        else:
            raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {input_path}")

    def get_video_metadata(self, video_path):
        """æå–è§†é¢‘åŸºç¡€ä¿¡æ¯"""
        try:
            cmd = [
                'ffprobe',
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_streams',
                '-show_format',
                video_path
            ]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if result.returncode != 0:
                raise RuntimeError(f"FFprobe failed: {result.stderr}")

            metadata = json.loads(result.stdout)

            video_info = {
                "filename": os.path.basename(video_path),
                "filepath": video_path,
                "duration": float(metadata['format']['duration']),
                "bit_rate": metadata['format'].get('bit_rate'),
                "streams": []
            }

            # æ·»åŠ ä¾¿äºè®¿é—®çš„å±æ€§
            video_info["width"] = 0
            video_info["height"] = 0
            video_info["has_audio"] = False

            for stream in metadata['streams']:
                if stream['codec_type'] == 'video':
                    fps_str = stream.get("r_frame_rate", "30/1")
                    fps = eval(fps_str) if fps_str else 30
                    video_info["width"] = stream.get("width", 0)
                    video_info["height"] = stream.get("height", 0)
                    video_info["streams"].append({
                        "type": "video",
                        "width": stream.get("width"),
                        "height": stream.get("height"),
                        "fps": fps,
                        "codec": stream.get("codec_name")
                    })
                elif stream['codec_type'] == 'audio':
                    video_info["has_audio"] = True
                    video_info["streams"].append({
                        "type": "audio",
                        "sample_rate": stream.get("sample_rate"),
                        "channels": stream.get("channels"),
                        "codec": stream.get("codec_name")
                    })

            return video_info
        except Exception as e:
            print(f"âŒ è·å–è§†é¢‘å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return None

    def detect_scenes(self, video_path, threshold=30.0):
        """åœºæ™¯æ£€æµ‹ - æ”¯æŒæ–°æ—§API"""
        try:
            if NEW_API_AVAILABLE is None:
                print("âŒ PySceneDetect ä¸å¯ç”¨ï¼Œè·³è¿‡åœºæ™¯æ£€æµ‹")
                return []

            if NEW_API_AVAILABLE:
                # ä½¿ç”¨æ–°ç‰ˆæœ¬ API
                print("[+] ä½¿ç”¨æ–°ç‰ˆ PySceneDetect API è¿›è¡Œåœºæ™¯æ£€æµ‹...")

                # æ‰“å¼€è§†é¢‘
                video = open_video(video_path)

                # åˆ›å»ºåœºæ™¯ç®¡ç†å™¨
                scene_manager = SceneManager()
                scene_manager.add_detector(ContentDetector(threshold=threshold))

                # æ£€æµ‹åœºæ™¯
                scene_manager.detect_scenes(video)
                scene_list = scene_manager.get_scene_list()

                # è½¬æ¢ä¸ºç§’æ•°æ ¼å¼
                scenes = [(start.get_seconds(), end.get_seconds()) for start, end in scene_list]

                print(f"[+] æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯")
                return scenes

            else:
                # ä½¿ç”¨æ—§ç‰ˆæœ¬ API (å‘åå…¼å®¹)
                print("[+] ä½¿ç”¨æ—§ç‰ˆ PySceneDetect API è¿›è¡Œåœºæ™¯æ£€æµ‹...")

                video_manager = VideoManager([video_path])
                scene_manager = SceneManager()
                scene_manager.add_detector(ContentDetector(threshold=threshold))

                base_timecode = video_manager.get_base_timecode()
                video_manager.set_downscale_factor()
                video_manager.start()
                scene_manager.detect_scenes(frame_source=video_manager)

                scene_list = scene_manager.get_scene_list(base_timecode)
                scenes = [(start.get_seconds(), end.get_seconds()) for start, end in scene_list]

                video_manager.release()
                print(f"[+] æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯")
                return scenes

        except Exception as e:
            print(f"âŒ åœºæ™¯æ£€æµ‹å¤±è´¥: {str(e)}")
            return []

    def extract_audio(self, video_path, output_audio="temp_audio.wav"):
        """æå–éŸ³é¢‘ - ä¼˜åŒ–ä¸ºWhisperæ ¼å¼"""
        try:
            # ä¸ºWhisperä¼˜åŒ–çš„éŸ³é¢‘æå–å‚æ•°
            cmd = [
                'ffmpeg', '-y', '-i', video_path,
                '-vn',  # ä¸è¦è§†é¢‘æµ
                '-acodec', 'pcm_s16le',  # éŸ³é¢‘ç¼–ç 
                '-ar', '16000',  # é‡‡æ ·ç‡16kHzï¼ˆWhisperæ¨èï¼‰
                '-ac', '1',  # å•å£°é“
                output_audio
            ]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            if result.returncode == 0:
                return output_audio
            else:
                print(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {result.stderr.decode()}")
                return None
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æå–å¼‚å¸¸: {str(e)}")
            return None

    def _format_time(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸º mm:ss.ff æ ¼å¼"""
        minutes = int(seconds // 60)
        remaining_seconds = seconds % 60
        return f"{minutes:02d}:{remaining_seconds:05.2f}"

    def _analyze_speech_segments(self, speech_timestamps: dict, target_duration: int = 30):
        """åŸºäºè¯­éŸ³æ—¶é—´æˆ³åˆ†ææœ€ä½³å‰ªè¾‘ç‰‡æ®µ"""

        segments = speech_timestamps.get("segments", [])
        if not segments:
            return []

        print(f"\nğŸ“Š è¯­éŸ³ç‰‡æ®µåˆ†æ (ç›®æ ‡æ—¶é•¿: {target_duration}ç§’):")
        print("-" * 50)

        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„è¯­éŸ³å¯†åº¦
        best_segments = []

        for i in range(0, len(segments)):
            # å°è¯•ä»å½“å‰æ®µå¼€å§‹ï¼Œæ‰¾åˆ°ç´¯è®¡æ—¶é•¿æ¥è¿‘ç›®æ ‡çš„ç‰‡æ®µç»„åˆ
            current_duration = 0
            current_segments = []

            for j in range(i, len(segments)):
                segment = segments[j]
                segment_duration = segment["end"] - segment["start"]

                if current_duration + segment_duration <= target_duration:
                    current_segments.append(segment)
                    current_duration += segment_duration
                else:
                    break

            if current_segments and current_duration >= target_duration * 0.8:  # è‡³å°‘80%çš„ç›®æ ‡æ—¶é•¿
                # è®¡ç®—è¿™ä¸ªç»„åˆçš„è´¨é‡åˆ†æ•°
                total_words = sum(len(seg["text"].split()) for seg in current_segments)
                density = total_words / max(current_duration, 1)  # è¯å¯†åº¦

                segment_info = {
                    "start": current_segments[0]["start"],
                    "end": current_segments[-1]["end"],
                    "duration": current_duration,
                    "segments_count": len(current_segments),
                    "word_density": density,
                    "text_preview": current_segments[0]["text"][:50] + "..."
                }

                best_segments.append(segment_info)

        # æŒ‰è¯å¯†åº¦æ’åºï¼Œé€‰æ‹©æœ€ä½³ç‰‡æ®µ
        best_segments.sort(key=lambda x: x["word_density"], reverse=True)

        print("ğŸ¯ æœ€ä½³è¯­éŸ³ç‰‡æ®µæ¨è:")
        for i, seg in enumerate(best_segments[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
            start_time = self._format_time(seg["start"])
            end_time = self._format_time(seg["end"])
            print(f"  {i + 1}. {start_time} - {end_time} "
                  f"(æ—¶é•¿: {seg['duration']:.1f}s, å¯†åº¦: {seg['word_density']:.1f} è¯/ç§’)")
            print(f"     å†…å®¹é¢„è§ˆ: {seg['text_preview']}")

        # å°†æœ€ä½³ç‰‡æ®µä¿å­˜åˆ°æ—¶é—´æˆ³æ•°æ®ä¸­
        speech_timestamps["best_segments"] = best_segments

        return best_segments

    def transcribe_audio(self, audio_path):
        """è¯­éŸ³è¯†åˆ« - ç®€åŒ–ç‰ˆï¼Œåªä¿ç•™å¥å­çº§æ—¶é—´æˆ³"""
        if not os.path.exists(audio_path):
            print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return ""

        try:
            # ä¼˜å…ˆä½¿ç”¨Whisperï¼ˆç¦»çº¿ï¼Œæ›´å‡†ç¡®ï¼‰
            if self.whisper_model:
                print("[+] ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«...")

                # å…³é”®ï¼šå¯ç”¨word_timestampså‚æ•°è·å–æ—¶é—´æˆ³
                result = self.whisper_model.transcribe(
                    audio_path,
                    word_timestamps=True,  # å¯ç”¨è¯çº§æ—¶é—´æˆ³
                    language='zh'  # æŒ‡å®šä¸­æ–‡
                )

                # æå–å®Œæ•´æ–‡æœ¬
                full_text = result["text"].strip()
                print(f"[+] Whisperè¯†åˆ«æˆåŠŸ: {full_text[:100]}...")

                # **ç®€åŒ–ç‰ˆï¼šåªæ˜¾ç¤ºå¥å­çº§æ—¶é—´æˆ³**
                print("\nğŸ™ï¸ è¯­éŸ³è¯†åˆ«å¥å­çº§æ—¶é—´æˆ³:")
                print("=" * 50)

                # **åªå¤„ç†segmentsï¼ˆå¥å­çº§åˆ«çš„æ—¶é—´æˆ³ï¼‰ï¼Œä¸å¤„ç†words**
                simplified_segments = []
                for i, segment in enumerate(result["segments"]):
                    start_time = round(segment["start"], 2)
                    end_time = round(segment["end"], 2)
                    text = segment["text"].strip()

                    # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
                    start_formatted = self._format_time(start_time)
                    end_formatted = self._format_time(end_time)

                    print(f"[{i + 1:2d}] {start_formatted} - {end_formatted} | {text}")

                    # **åªä¿å­˜å¥å­çº§ä¿¡æ¯ï¼Œä¸ä¿å­˜è¯çº§ä¿¡æ¯**
                    simplified_segments.append({
                        "start": start_time,
                        "end": end_time,
                        "text": text,
                        "duration": round(end_time - start_time, 2)
                    })

                print("=" * 50)

                # **ç®€åŒ–çš„æ—¶é—´æˆ³å­—å…¸ï¼ŒåªåŒ…å«å¿…è¦ä¿¡æ¯**
                timestamp_data = {
                    "full_text": full_text,
                    "segments": simplified_segments,  # åªä¿ç•™å¥å­çº§åˆ«
                    "total_duration": simplified_segments[-1]["end"] if simplified_segments else 0,
                    "segments_count": len(simplified_segments)
                }

                # å°†ç®€åŒ–çš„æ—¶é—´æˆ³æ•°æ®ä¿å­˜åˆ°å®ä¾‹ä¸­
                self.speech_timestamps = timestamp_data

                # åˆ†ææœ€ä½³è¯­éŸ³ç‰‡æ®µ
                self._analyze_speech_segments(timestamp_data)

                # **æ‰“å°æœ€ç»ˆä¿å­˜çš„æ—¶é—´æˆ³æ•°æ®å¤§å°ï¼ˆç”¨äºè°ƒè¯•ï¼‰**
                print(f"\nğŸ“Š æ—¶é—´æˆ³æ•°æ®æ‘˜è¦:")
                print(f"  - å¥å­æ•°é‡: {len(simplified_segments)}")
                print(f"  - æ€»æ—¶é•¿: {timestamp_data['total_duration']:.1f}ç§’")
                print(
                    f"  - å¹³å‡å¥é•¿: {len(full_text) / len(simplified_segments) if simplified_segments else 0:.1f}å­—/å¥")

                if full_text:
                    return full_text
                else:
                    print("[!] Whisperè¯†åˆ«ç»“æœä¸ºç©º")
            else:
                print("[!] Whisperæ¨¡å‹ä¸å¯ç”¨")

        except Exception as e:
            print(f"âŒ Whisperè¯†åˆ«å¤±è´¥: {e}")

        # Fallbackåˆ°Googleï¼ˆä¿æŒåŸæœ‰å…¼å®¹æ€§ï¼‰
        return self._fallback_google_recognition(audio_path)

    def _fallback_google_recognition(self, audio_path):
        """å¤‡ç”¨çš„Googleè¯†åˆ«"""
        try:
            print("[+] å°è¯•Googleè¯­éŸ³è¯†åˆ«...")
            recognizer = sr.Recognizer()
            with sr.AudioFile(audio_path) as source:
                audio = recognizer.record(source)
            text = recognizer.recognize_google(audio, language='zh-CN')
            print(f"[+] Googleè¯†åˆ«æˆåŠŸ: {text[:50]}...")
            return text
        except Exception as e:
            print(f"âŒ Googleè¯†åˆ«å¤±è´¥: {e}")
            return ""

    def analyze_background_music(self, audio_path):
        """èƒŒæ™¯éŸ³ä¹åˆ†æ"""
        if not os.path.exists(audio_path):
            return {"tempo": 0, "energy": 0, "chroma_mean": 0}

        try:
            y, sr_rate = librosa.load(audio_path, sr=None)
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr_rate)
            chroma = librosa.feature.chroma_stft(y=y, sr=sr_rate).mean()
            energy = np.mean(librosa.feature.rms(y=y))
            return {
                "tempo": round(float(tempo)),
                "energy": round(float(energy), 2),
                "chroma_mean": round(float(chroma), 2)
            }
        except Exception as e:
            print(f"âŒ éŸ³ä¹åˆ†æå¤±è´¥: {str(e)}")
            return {"tempo": 0, "energy": 0, "chroma_mean": 0}

    def detect_objects(self, video_path, conf=0.5):
        """å¯¹è±¡æ£€æµ‹"""
        try:
            if self.yolo_model is None:
                print("[+] åŠ è½½YOLOæ¨¡å‹...")
                self.yolo_model = YOLO('yolov8n.pt')  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹

            results = self.yolo_model(video_path, stream=True, conf=conf)
            detected_objects = {}

            frame_count = 0
            max_frames = 30  # åªæ£€æµ‹å‰30å¸§ï¼Œæé«˜é€Ÿåº¦

            for r in results:
                if frame_count >= max_frames:
                    break
                frame_count += 1

                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        cls = int(box.cls[0])
                        name = self.yolo_model.names[cls]
                        confidence = float(box.conf[0])

                        if name not in detected_objects:
                            detected_objects[name] = []
                        detected_objects[name].append(confidence)

            # è®¡ç®—æ¯ä¸ªå¯¹è±¡çš„å¹³å‡ç½®ä¿¡åº¦
            object_summary = {}
            for obj_name, confidences in detected_objects.items():
                object_summary[obj_name] = {
                    "count": len(confidences),
                    "avg_confidence": round(sum(confidences) / len(confidences), 2),
                    "max_confidence": round(max(confidences), 2)
                }

            return object_summary
        except Exception as e:
            print(f"âŒ å¯¹è±¡æ£€æµ‹å¤±è´¥: {str(e)}")
            return {}

    def detect_faces(self, video_path):
        """äººè„¸æ£€æµ‹"""
        try:
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            cap = cv2.VideoCapture(video_path)
            face_present = False

            frame_count = 0
            max_frames = 30  # åªæ£€æµ‹å‰30å¸§

            while cap.isOpened() and frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break

                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                if len(faces) > 0:
                    face_present = True
                    break
                frame_count += 1

            cap.release()
            return face_present
        except Exception as e:
            print(f"âŒ äººè„¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return False

    def classify_video_content(self, analysis_report):
        """è§†é¢‘å†…å®¹åˆ†ç±»"""
        content_type = "unknown"
        mood = "neutral"
        structure = "linear"
        style = "standard"
        purpose = "general"

        speech_text = analysis_report.get("speech_text", "")
        has_face = analysis_report.get("face_detected", False)
        detected_objects = analysis_report.get("objects_detected", {})
        scene_changes = analysis_report.get("scene_changes", [])
        music_analysis = analysis_report.get("music_analysis", {})
        tempo = music_analysis.get("tempo", 0)
        duration = analysis_report.get("metadata", {}).get("duration", 0)

        # 1. åˆ¤æ–­content_type
        if len(speech_text.strip()) > 0 or has_face:
            content_type = "äººå£°å‰§æƒ…ç±»"
            if 'person' in detected_objects:
                if duration > 1800:  # 30åˆ†é’Ÿä»¥ä¸Š
                    content_type = "ç›´æ’­å½•æ’­ç±»"
                elif any(keyword in speech_text for keyword in ["äº§å“", "è´­ä¹°", "ä¼˜æƒ ", "è¯¾ç¨‹", "æ•™å­¦", "åŸ¹è®­"]):
                    content_type = "å¹¿å‘Šå®£ä¼ ç‰‡ç±»"
                elif any(keyword in speech_text for keyword in ["æ•™", "å­¦", "æ­¥éª¤", "æ–¹æ³•"]):
                    content_type = "æ•™è‚²æ•™å­¦ç±»"
        else:
            content_type = "åœºæ™¯é£æ™¯ç±»"
            if any(obj in detected_objects for obj in ['car', 'bicycle', 'skateboard', 'sports ball']):
                content_type = "åŠ¨ä½œè¿åŠ¨ç±»"
            elif tempo > 100 and 'person' not in detected_objects:
                content_type = "éŸ³ä¹è§†é¢‘ç±»"

        # 2. åˆ¤æ–­mood
        if tempo > 120:
            mood = "æ¿€æ˜‚"
        elif tempo < 80 and len(scene_changes) < 5:
            mood = "å®é™"
        elif any(keyword in speech_text for keyword in ["æ„ŸåŠ¨", "éš¾è¿‡", "æ³ª"]):
            mood = "æ„ŸåŠ¨"
        elif any(keyword in speech_text for keyword in ["å±é™©", "ç´§æ€¥", "å¿«", "é™æ—¶", "é©¬ä¸Š", "ç«‹å³"]):
            mood = "ç´§å¼ "
        elif any(keyword in speech_text for keyword in ["å¼€å¿ƒ", "å¿«ä¹", "å“ˆå“ˆ"]):
            mood = "æ¬¢å¿«"

        # 3. åˆ¤æ–­structure
        if len(scene_changes) > 20:
            structure = "æ¿å—æ‹¼æ¥"
        elif len(scene_changes) > 10:
            structure = "å¤šçº¿å¹¶è¡Œ"
        elif any(keyword in speech_text for keyword in ["å›å¿†", "è¿‡å»", "ä»¥å‰"]):
            structure = "å›å¿†æ’å™"

        # 4. åˆ¤æ–­style
        metadata = analysis_report.get("metadata", {})
        width = metadata.get("width", 1920)
        height = metadata.get("height", 1080)

        if height > width:  # ç«–å±
            style = "æŠ–éŸ³é£"
        elif duration > 1800:  # é•¿è§†é¢‘
            style = "çºªå½•ç‰‡é£"
        elif tempo > 120 and len(scene_changes) > 15:
            style = "æŠ–éŸ³é£"
        elif has_face and duration < 600:  # æœ‰äººè„¸ä¸”10åˆ†é’Ÿå†…
            style = "Vlogé£"
        elif 'cinematic' in speech_text.lower():
            style = "ç”µå½±æ„Ÿ"

        # 5. åˆ¤æ–­purpose
        if duration < 60:
            purpose = "ç¤¾äº¤åª’ä½“"
        elif any(keyword in speech_text for keyword in ["æ•™ç¨‹", "æ•™ä½ ", "å­¦ä¹ ", "æ­¥éª¤"]):
            purpose = "æ•™å­¦åŸ¹è®­"
        elif any(keyword in speech_text for keyword in ["äº§å“", "å…¬å¸", "å“ç‰Œ", "æœåŠ¡"]):
            purpose = "ä¼ä¸šå®£ä¼ "
        elif any(keyword in speech_text for keyword in ["æ—…è¡Œ", "ç”Ÿæ´»", "æ—¥å¸¸", "åˆ†äº«"]):
            purpose = "ä¸ªäººè®°å½•"

        return {
            "content_type": content_type,
            "mood": mood,
            "structure": structure,
            "style": style,
            "purpose": purpose
        }

    def generate_highlights(self, analysis_report):
        """ç”Ÿæˆç²¾å½©ç‰‡æ®µæ¨è"""
        highlights = []
        scene_changes = analysis_report.get("scene_changes", [])
        duration = analysis_report.get("metadata", {}).get("duration", 0)

        # åŸºäºåœºæ™¯å˜åŒ–æ¨èç²¾å½©ç‰‡æ®µ
        if scene_changes:
            # é€‰æ‹©å‰å‡ ä¸ªåœºæ™¯ä½œä¸ºç²¾å½©ç‰‡æ®µ
            for i, (start, end) in enumerate(scene_changes[:5]):
                highlights.append({
                    "start_time": start,
                    "end_time": end,
                    "duration": end - start,
                    "reason": f"åœºæ™¯ {i + 1}",
                    "confidence": 0.7
                })

        # å¦‚æœæ²¡æœ‰åœºæ™¯å˜åŒ–ï¼ŒæŒ‰æ—¶é—´æ®µæ¨è
        if not highlights and duration > 0:
            segment_duration = min(30, duration / 3)  # æ¯æ®µæœ€å¤š30ç§’
            for i in range(3):
                start = i * (duration / 3)
                end = min(start + segment_duration, duration)
                if end > start:
                    highlights.append({
                        "start_time": start,
                        "end_time": end,
                        "duration": end - start,
                        "reason": f"æ—¶é—´æ®µ {i + 1}",
                        "confidence": 0.5
                    })

        return highlights

    def _save_timestamps_to_file(self, video_path: str, analysis_result: dict):
        """ä¿å­˜ç®€åŒ–çš„æ—¶é—´æˆ³ä¿¡æ¯åˆ°æ–‡ä»¶"""
        timestamp_file = video_path.replace('.mkv', '_timestamps.json').replace('.mp4', '_timestamps.json')
        try:
            # **åªä¿å­˜å¿…è¦çš„ä¿¡æ¯ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§**
            timestamp_data = {
                'video_path': video_path,
                'analysis_time': __import__('datetime').datetime.now().isoformat(),
                'speech_summary': {
                    'total_duration': analysis_result.get('speech_timestamps', {}).get('total_duration', 0),
                    'segments_count': analysis_result.get('speech_timestamps', {}).get('segments_count', 0),
                    'text_length': len(analysis_result.get('speech_text', ''))
                },
                'highlights': analysis_result.get('highlights', []),
                'classification': analysis_result.get('classification', {}),
                'best_segments': analysis_result.get('speech_timestamps', {}).get('best_segments', [])[:3]  # åªä¿å­˜å‰3ä¸ªæœ€ä½³ç‰‡æ®µ
            }

            with open(timestamp_file, 'w', encoding='utf-8') as f:
                json.dump(timestamp_data, f, ensure_ascii=False, indent=2)
            print(f"[+] ç®€åŒ–æ—¶é—´æˆ³ä¿¡æ¯å·²ä¿å­˜åˆ°: {timestamp_file}")
        except Exception as e:
            print(f"[!] ä¿å­˜æ—¶é—´æˆ³æ–‡ä»¶å¤±è´¥: {e}")

    def get_best_speech_segments(self, target_duration: int = 30):
        """è·å–æœ€ä½³è¯­éŸ³ç‰‡æ®µï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        if hasattr(self, 'speech_timestamps') and self.speech_timestamps:
            return self.speech_timestamps.get('best_segments', [])
        return []

    def analyze_video(self, video_path):
        """ç»¼åˆè§†é¢‘åˆ†æ - ç®€åŒ–è¾“å‡ºç‰ˆ"""
        print(f"[+] æ­£åœ¨åˆ†æè§†é¢‘: {video_path}")

        print("[+] æ­£åœ¨æå–åŸºç¡€ä¿¡æ¯...")
        metadata = self.get_video_metadata(video_path)
        if not metadata:
            raise ValueError("æ— æ³•è·å–è§†é¢‘å…ƒæ•°æ®")

        print("[+] æ­£åœ¨è¿›è¡Œåœºæ™¯æ£€æµ‹...")
        scenes = self.detect_scenes(video_path)

        print("[+] æå–éŸ³é¢‘å¹¶è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
        audio_path = self.extract_audio(video_path)
        speech_text = ""
        music_analysis = {"tempo": 120, "energy": 0.01, "chroma_mean": 0.3}

        if audio_path:
            speech_text = self.transcribe_audio(audio_path)  # è¿™é‡Œä¼šç”Ÿæˆç®€åŒ–çš„æ—¶é—´æˆ³
            music_analysis = self.analyze_background_music(audio_path)
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            try:
                os.remove(audio_path)
            except:
                pass
        else:
            print("[!] æ— æ³•æå–éŸ³é¢‘ï¼Œè·³è¿‡è¯­éŸ³ç›¸å…³åˆ†æ")

        print("[+] è¿›è¡Œå¯¹è±¡æ£€æµ‹...")
        objects = self.detect_objects(video_path)

        print("[+] æ£€æµ‹æ˜¯å¦æœ‰äººè„¸...")
        has_face = self.detect_faces(video_path)

        # æ„å»ºç®€åŒ–çš„åˆ†ææŠ¥å‘Š
        analysis_report = {
            "metadata": metadata,
            "scene_changes": scenes,
            "speech_text": speech_text,
            "speech_timestamps": getattr(self, 'speech_timestamps', {}),  # ç®€åŒ–çš„æ—¶é—´æˆ³æ•°æ®
            "music_analysis": music_analysis,
            "objects_detected": objects,
            "face_detected": has_face
        }

        # è¿›è¡Œå†…å®¹åˆ†ç±»
        print("[+] è¿›è¡Œå†…å®¹åˆ†ç±»...")
        classification = self.classify_video_content(analysis_report)
        analysis_report["classification"] = classification

        # ç”Ÿæˆç²¾å½©ç‰‡æ®µ
        highlights = self.generate_highlights(analysis_report)
        analysis_report["highlights"] = highlights

        # ä¿å­˜ç®€åŒ–çš„æ—¶é—´æˆ³åˆ°æ–‡ä»¶
        self._save_timestamps_to_file(video_path, analysis_report)

        print("âœ… è§†é¢‘åˆ†æå®Œæˆ")

        # **ç®€åŒ–è¾“å‡ºï¼šä¸æ˜¾ç¤ºå®Œæ•´çš„æ—¶é—´æˆ³æ•°æ®**
        print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        print(f"  - è§†é¢‘æ—¶é•¿: {metadata.get('duration', 0):.1f}ç§’")
        print(f"  - å†…å®¹ç±»å‹: {classification.get('content_type', 'æœªçŸ¥')}")
        print(f"  - è¯­éŸ³å¥å­æ•°: {len(analysis_report.get('speech_timestamps', {}).get('segments', []))}")
        print(f"  - ç²¾å½©ç‰‡æ®µæ•°: {len(highlights)}")

        return analysis_report


# ä¾¿æ·å‡½æ•°
def analyze_video(video_path):
    """å¿«é€Ÿåˆ†æå•ä¸ªè§†é¢‘çš„ä¾¿æ·å‡½æ•°"""
    analyzer = VideoAnalyzer()
    return analyzer.analyze_video(video_path)


def detect_videos(input_path):
    """å¿«é€Ÿæ£€æµ‹è§†é¢‘æ–‡ä»¶"""
    analyzer = VideoAnalyzer()
    return analyzer.detect_videos(input_path)