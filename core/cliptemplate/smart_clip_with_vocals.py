import os
from typing import List

import numpy as np
import cv2
import librosa
import webrtcvad
from moviepy import VideoFileClip, concatenate_videoclips, AudioFileClip, CompositeVideoClip, vfx,VideoClip
from pydub import AudioSegment
from skimage import filters, morphology
import wave
from core.cliptemplate.random_transition import apply_random_transition
from core.cliptransition.easy_clip_transitions import black_transition
import uuid

# =================== 1. è§†é¢‘ä¸éŸ³é¢‘å¤„ç† ===================

def load_video_and_audio(video_path):
    """åŠ è½½è§†é¢‘å’ŒéŸ³é¢‘"""
    video = VideoFileClip(video_path)
    audio_path = "temp_audio.wav"
    video.audio.write_audiofile(audio_path)
    return video, audio_path

def process_audio_for_vad(audio_path):
    """å°†éŸ³é¢‘è½¬æ¢ä¸ºå•å£°é“16kHzï¼Œä¾›VADä½¿ç”¨"""
    audio = AudioSegment.from_wav(audio_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    processed_path = "processed_audio.wav"
    audio.export(processed_path, format="wav")
    return processed_path

# =================== 2. äººå£°æ£€æµ‹ ===================

def detect_speech_webrtcvad(audio_path):
    """ä½¿ç”¨WebRTC VADæ£€æµ‹äººå£°ç‰‡æ®µ"""
    vad = webrtcvad.Vad()
    vad.set_mode(3)  # æ¨¡å¼3ï¼šæœ€ä¸¥æ ¼ï¼Œé€‚åˆå˜ˆæ‚ç¯å¢ƒ

    speech_segments = []

    with wave.open(audio_path, 'rb') as wf:
        # ç¡®ä¿éŸ³é¢‘æ ¼å¼ç¬¦åˆè¦æ±‚
        if wf.getnchannels() != 1:
            raise ValueError("Audio must be mono")
        if wf.getsampwidth() != 2:
            raise ValueError("Audio must be 16-bit PCM")
        if wf.getframerate() not in (8000, 16000, 32000):
            raise ValueError("Sample rate must be 8000, 16000 or 32000")

        sample_rate = wf.getframerate()
        frame_duration_ms = 30  # å¯é€‰ 10ã€20 æˆ– 30 æ¯«ç§’
        frame_size_bytes = int(sample_rate * (frame_duration_ms / 1000) * 2)  # æ¯å¸§å­—èŠ‚æ•° = æ ·æœ¬æ•° Ã— 2å­—èŠ‚

        current_time = 0.0
        frame_count = 0
        is_previous_speech = False
        segment_start = None

        while True:
            frame_bytes = wf.readframes(frame_size_bytes // 2)  # readframes å‚æ•°æ˜¯æ ·æœ¬æ•°
            if len(frame_bytes) < frame_size_bytes:
                break  # ç»“æŸ

            is_speech = vad.is_speech(frame_bytes, sample_rate)

            if is_speech:
                if not is_previous_speech:
                    segment_start = current_time
                is_previous_speech = True
            else:
                if is_previous_speech:
                    speech_segments.append((segment_start, current_time))
                is_previous_speech = False

            current_time += frame_duration_ms / 1000.0
            frame_count += 1

        # å¦‚æœæœ€åä¸€æ®µæ˜¯è¯­éŸ³ï¼Œä¹Ÿè¦åŠ è¿›å»
        if is_previous_speech:
            speech_segments.append((segment_start, current_time))

    return speech_segments

# =================== 3. é«˜æ½®ç‰‡æ®µæ£€æµ‹ ===================

def detect_audio_peaks(audio_path, sr=16000):
    """æ£€æµ‹éŸ³é¢‘èƒ½é‡çªå˜"""
    y, _ = librosa.load(audio_path, sr=sr)
    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)
    rms_db = librosa.amplitude_to_db(rms, ref=np.max)
    rms_diff = np.diff(rms_db[0])
    mean_diff = np.mean(np.abs(rms_diff))
    threshold = 2 * mean_diff
    peak_frames = np.where(np.abs(rms_diff) > threshold)[0]
    peak_times = librosa.frames_to_time(peak_frames, sr=sr, hop_length=512)
    return peak_times

def detect_visual_motion(video_path):
    """æ£€æµ‹è§†è§‰è¿åŠ¨å¼ºåº¦"""
    cap = cv2.VideoCapture(video_path)
    ret, frame1 = cap.read()
    ret, frame2 = cap.read()
    motion_times = []
    count = 0
    while cap.isOpened():
        ret, frame3 = cap.read()
        if not ret:
            break
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        gray3 = cv2.cvtColor(frame3, cv2.COLOR_BGR2GRAY)
        diff1 = cv2.absdiff(gray1, gray2)
        diff2 = cv2.absdiff(gray2, gray3)
        diff = cv2.bitwise_and(diff1, diff2)
        _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        thresh = cv2.dilate(thresh, kernel, iterations=2)
        motion_strength = np.mean(thresh)
        if motion_strength > 100:
            motion_times.append(count / cap.get(cv2.CAP_PROP_FPS))
        frame1 = frame2
        frame2 = frame3
        count += 1
    cap.release()
    return motion_times

# =================== 4. æƒé‡è®¡ç®— ===================

def segment_video(video, segment_duration=1.0):
    """å°†è§†é¢‘æŒ‰å›ºå®šæ—¶é•¿åˆ‡åˆ†ç‰‡æ®µ"""
    duration = video.duration
    segments = []
    for i in range(int(duration // segment_duration) + 1):
        start = i * segment_duration
        end = min((i + 1) * segment_duration, duration)
        if start >= end:
            break
        segments.append((start, end))
    return segments

def calculate_weights(segments, speech_times, peak_times, motion_times):
    """è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„æƒé‡"""
    weights = []
    for start, end in segments:
        # è®¡ç®—äººå£°æƒé‡
        speech_duration = 0
        for s_start, s_end in speech_times:
            overlap_start = max(start, s_start)
            overlap_end = min(end, s_end)
            if overlap_start < overlap_end:
                speech_duration += overlap_end - overlap_start
        speech_weight = 0.4 * (1 + 0.1 * (speech_duration / (end - start))) if (end - start) > 0 else 0
        
        # è®¡ç®—é«˜æ½®æƒé‡ï¼ˆéŸ³é¢‘+è§†è§‰ï¼‰
        peak_count = sum(1 for t in peak_times if start <= t <= end)
        motion_count = sum(1 for t in motion_times if start <= t <= end)
        peak_weight = 0.6 * (1 + 0.2 * (peak_count + motion_count) / (end - start)) if (end - start) > 0 else 0
        
        total_weight = speech_weight + peak_weight
        weights.append((start, end, total_weight))
    return weights

# =================== 5. è´ªå¿ƒç®—æ³•ç­›é€‰ç‰‡æ®µ ===================

def greedy_select_segments(weights, min_gap=0.5,amount=100):
    """è´ªå¿ƒç®—æ³•é€‰æ‹©äº’ä¸å†²çªçš„é«˜æƒé‡ç‰‡æ®µ"""
    # æŒ‰æƒé‡é™åºæ’åº
    sorted_weights = sorted(weights, key=lambda x: x[2], reverse=True)
    selected = []
    last_end = 0
    # for start, end, weight in sorted_weights:
    #     if start >= last_end:
    #         selected.append((start, end))
    #         last_end = end
    #     elif end - last_end < min_gap:
    #         continue  # ç¡®ä¿ç‰‡æ®µé—´æœ‰ä¸€å®šé—´éš”
    #     if len(selected) >= amount:  # å‰ªåˆ‡æ•°é‡è¾¾åˆ°è¦æ±‚ï¼Œç»“æŸå¾ªç¯
    #         break
    return sorted_weights[0:amount]

# =================== 6. å‰ªè¾‘ä¸è¾“å‡º ===================

def apply_transition(clip1:VideoClip, clip2:VideoClip, motion_strength,duration=0.5):
    """æ ¹æ®è¿åŠ¨å¼ºåº¦é€‰æ‹©è½¬åœºæ•ˆæœ"""
    # if motion_strength > 150:
    #     transition = clip1.with_effects([vfx.FadeOut(duration)])
    # elif motion_strength > 100:
    #     transition = CompositeVideoClip([clip1.subclipped(start_time=clip1.duration-duration,end_time=duration), clip2.with_opacity(0.5).subclipped(start_time=0, end_time=duration)])
    # else:
    #     transition = CompositeVideoClip([clip1.subclipped(start_time=clip1.duration-duration,end_time=duration), clip2.with_opacity(0.7).subclipped(start_time=0, end_time=duration)])
    transition = apply_random_transition(clip1, clip2, duration=duration)
    # return concatenate_videoclips([clip1, clip2])
    return transition

def smart_cut_video(videos, selected_segments, output_path):
    """æ ¹æ®é€‰ä¸­çš„ç‰‡æ®µå‰ªè¾‘è§†é¢‘"""
    clips = []
    # for i, (start, end) in enumerate(selected_segments):
    #     clip = video.subclipped(start, end)
    #     if i > 0:
    #         # è·å–ä¸Šä¸€ä¸ªç‰‡æ®µçš„è¿åŠ¨å¼ºåº¦ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰
    #         motion_strength = 120  # å¯æ ¹æ®å®é™…æ£€æµ‹ç»“æœæ›¿æ¢
    #         transition_clip = apply_transition(clips[-1], clip, motion_strength)
    #         clips[-1] = transition_clip
    #     #     clips.append(transition_clip)
    #     #     apply_random_transition()
    #     else:
    #         clips.append(clip)
    for i in range(len(selected_segments)):
        if i>0:
            clip=videos.subclipped(selected_segments[i][0], selected_segments[i][1])
            # clips.append(clip)
            # clip1=videos.subclipped(selected_segments[i+1][0], selected_segments[i+1][1])
            transition = apply_random_transition(clips, clip)

            clips=transition

        else:
            clip = videos.subclipped(selected_segments[i][0], selected_segments[i][1])
            clips=clip

            # transition = black_transition(clip)
            # clips.append(transition)



    # final_clip = concatenate_videoclips(clips)
    final_clip=clips
    final_clip.write_videofile(output_path, codec="libx264")

# =================== 7. ä¸»æµç¨‹ ===================

def smart_clip(video_path, output_path):
    # 1. åŠ è½½è§†é¢‘å’ŒéŸ³é¢‘
    video, audio_path = load_video_and_audio(video_path)
    
    # 2. å¤„ç†éŸ³é¢‘å¹¶æ£€æµ‹äººå£°
    processed_audio_path = process_audio_for_vad(audio_path)
    speech_times = detect_speech_webrtcvad(processed_audio_path)
    
    # 3. æ£€æµ‹é«˜æ½®ç‰‡æ®µ
    peak_times = detect_audio_peaks(audio_path)
    motion_times = detect_visual_motion(video_path)
    
    # 4. åˆ‡åˆ†è§†é¢‘ç‰‡æ®µå¹¶è®¡ç®—æƒé‡
    segments = segment_video(video, segment_duration=3.0)  # 3ç§’ä¸€æ®µ
    weights = calculate_weights(segments, speech_times, peak_times, motion_times)
    
    # 5. è´ªå¿ƒç®—æ³•é€‰æ‹©é«˜æƒé‡ç‰‡æ®µ
    selected_segments = greedy_select_segments(weights,amount=5)
    
    # 6. å‰ªè¾‘å¹¶è¾“å‡º
    smart_cut_video(video, selected_segments, output_path)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(audio_path)
    os.remove(processed_audio_path)


def smart_clips(
        enterprise_files: List[str],
        audio_clips: List,
        project_dir: str,
        num_clips: int
) -> List:
    """
    æ™ºèƒ½å‰ªè¾‘å‡½æ•° - æ ¹æ®éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå¯¹åº”çš„è§†é¢‘ç‰‡æ®µ

    å‚æ•°:
        enterprise_files: ä¼ä¸šç´ ææ–‡ä»¶è·¯å¾„åˆ—è¡¨
        audio_clips: éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨
        project_dir: é¡¹ç›®ç›®å½•è·¯å¾„
        num_clips: éœ€è¦ç”Ÿæˆçš„ç‰‡æ®µæ•°é‡

    è¿”å›:
        List: å¤„ç†å¥½çš„è§†é¢‘ç‰‡æ®µåˆ—è¡¨
    """
    from moviepy import VideoFileClip, concatenate_videoclips, CompositeVideoClip
    import random
    import os

    print(f"ğŸ¬ å¼€å§‹æ™ºèƒ½å‰ªè¾‘å¤„ç†...")
    print(f"   ä¼ä¸šç´ ææ•°é‡: {len(enterprise_files)}")
    print(f"   éŸ³é¢‘ç‰‡æ®µæ•°é‡: {len(audio_clips)}")
    print(f"   éœ€è¦ç”Ÿæˆç‰‡æ®µ: {num_clips}")

    if not enterprise_files:
        raise ValueError("ä¼ä¸šç´ ææ–‡ä»¶åˆ—è¡¨ä¸ºç©º")

    if len(audio_clips) != num_clips:
        print(f"âš ï¸ éŸ³é¢‘ç‰‡æ®µæ•°é‡({len(audio_clips)})ä¸éœ€è¦ç”Ÿæˆçš„ç‰‡æ®µæ•°é‡({num_clips})ä¸åŒ¹é…")

    # åˆå§‹åŒ–è§†é¢‘ç‰‡æ®µåˆ—è¡¨
    enterprise_clips = []

    try:
        # ä¸ºæ¯ä¸ªéŸ³é¢‘ç‰‡æ®µç”Ÿæˆå¯¹åº”çš„è§†é¢‘ç‰‡æ®µ
        for idx, audio_clip in enumerate(audio_clips[:num_clips]):
            print(f"ğŸ¯ å¤„ç†ç¬¬ {idx + 1}/{num_clips} ä¸ªç‰‡æ®µ...")

            # éšæœºé€‰æ‹©ä¸€ä¸ªä¼ä¸šç´ ææ–‡ä»¶
            video_path = random.choice(enterprise_files)
            print(f"   é€‰ä¸­ç´ æ: {os.path.basename(video_path)}")

            # åŠ è½½è§†é¢‘
            video_clip = VideoFileClip(video_path)

            # è°ƒæ•´è§†é¢‘å°ºå¯¸
            if video_clip.size[0] > video_clip.size[1]:
                # æ¨ªå±è§†é¢‘
                video_clip = video_clip.resized((1920, 1080))
            else:
                # ç«–å±è§†é¢‘
                video_clip = video_clip.resized((1080, 1920))

            # è·å–éŸ³é¢‘é•¿åº¦
            target_duration = audio_clip.duration
            print(f"   ç›®æ ‡æ—¶é•¿: {target_duration:.2f}ç§’")

            # è°ƒæ•´è§†é¢‘é•¿åº¦åŒ¹é…éŸ³é¢‘
            if video_clip.duration > target_duration:
                # è§†é¢‘è¾ƒé•¿ï¼Œéšæœºæˆªå–ç‰‡æ®µ
                max_start = video_clip.duration - target_duration - 0.1
                start_time = random.uniform(0, max(0, max_start))
                video_clip = video_clip.subclipped(start_time, start_time + target_duration)
                print(f"   æˆªå–ç‰‡æ®µ: {start_time:.2f}s - {start_time + target_duration:.2f}s")
            else:
                # è§†é¢‘è¾ƒçŸ­ï¼Œå¾ªç¯æ’­æ”¾
                print(f"   è§†é¢‘è¾ƒçŸ­({video_clip.duration:.2f}s)ï¼Œéœ€è¦å¾ªç¯æ’­æ”¾")
                try:
                    # è®¡ç®—éœ€è¦å¾ªç¯çš„æ¬¡æ•°
                    loop_count = int(target_duration / video_clip.duration) + 1

                    # æ‰‹åŠ¨åˆ›å»ºå¾ªç¯
                    looped_clips = []
                    remaining_duration = target_duration

                    while remaining_duration > 0:
                        if remaining_duration >= video_clip.duration:
                            # å®Œæ•´å¾ªç¯
                            looped_clips.append(video_clip)
                            remaining_duration -= video_clip.duration
                        else:
                            # éƒ¨åˆ†å¾ªç¯
                            looped_clips.append(video_clip.subclipped(0, remaining_duration))
                            remaining_duration = 0

                    video_clip = concatenate_videoclips(looped_clips, method="compose")
                    print(f"   å¾ªç¯å®Œæˆï¼Œæœ€ç»ˆæ—¶é•¿: {video_clip.duration:.2f}s")

                except Exception as loop_error:
                    print(f"   âš ï¸ å¾ªç¯å¤„ç†å¤±è´¥: {loop_error}")
                    # å¦‚æœå¾ªç¯å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸè§†é¢‘å¹¶è°ƒæ•´åˆ°ç›®æ ‡æ—¶é•¿
                    if video_clip.duration < target_duration:
                        # ç®€å•é‡å¤æœ€åä¸€å¸§
                        video_clip = video_clip.with_duration(target_duration)
                    else:
                        video_clip = video_clip.subclipped(0, target_duration)

            # ç»‘å®šéŸ³é¢‘åˆ°è§†é¢‘
            video_clip = video_clip.with_audio(audio_clip)
            enterprise_clips.append(video_clip)

            print(f"   âœ… ç¬¬ {idx + 1} ä¸ªç‰‡æ®µå¤„ç†å®Œæˆ")

        print(f"ğŸ‰ æ™ºèƒ½å‰ªè¾‘å®Œæˆï¼Œç”Ÿæˆ {len(enterprise_clips)} ä¸ªç‰‡æ®µ")
        return enterprise_clips

    except Exception as e:
        print(f"âŒ æ™ºèƒ½å‰ªè¾‘å¤„ç†å¤±è´¥: {e}")
        # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†å›é€€é€»è¾‘
        raise

# =================== 8. è¿è¡Œç¤ºä¾‹ ===================

if __name__ == "__main__":
    # input_video = "/Users/luming/PycharmProjects/AI-Movie-Clip/cliptest/è£…ä¿®è¡Œä¸š/è£…ä¿®1.mp4"
    # output_video = "output_smart_cut.mp4"
    # smart_clip(input_video, output_video)

    directory_path = "//cliptest/è£…ä¿®è¡Œä¸š"
    smart_clips(directory_path, "output10.mp4", is_directory=True)

    # æ‰“å°ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆè°ƒè¯•ç”¨ï¼‰
    # directory_path = "/Users/luming/PycharmProjects/AI-Movie-Clip/cliptest/è£…ä¿®è¡Œä¸š"
    # print("ç›®å½•ä¸­çš„æ–‡ä»¶ï¼š")
    # for file in os.listdir(directory_path):
    #     print(f"- {file}")